run:
  problem:  chained_serpentine           # es. quad, ext_powell (entrambi presi da [1])
  n_value: 1000                         # richiesto dall'assignment [2, 1000, 10000, 100000]
  methods:  'mn'                       # {mn, tn} ufficiali; gd solo extra se vuoi
  seed: 349308                          # == lowest student ID
  start_type: xbar        # {xbar, random}
  random_id: 0            # IMPOSTARE 0 SE SI USA xbar, sennò 1,2,3,4,5 per ogni random
  max_iters: 10000                     # 1000–2000 è ok per report
  tolerance: 1e-6                     # ||grad|| < tol
  num_random_starts: 5                # 5 starting points random + uno x̄ di default
  save_paths_2d: true                 # salva traiettorie per n = 2 per i plot
  save_rates: true                    # salva dati per experimental rate of convergence

line_search:
  type: armijo
  alpha0: 1.0        # initial step length
  rho: 0.5        # step reduction factor
  c: 1e-4            # Armijo parameter
  max_ls_iter: 20

derivatives:
  mode: exact       # {exact, fd_hessian, fd_all, exact_grad_fd_hessian} → per gestire i vari esperimenti
  # per finite differences:
  h_exponents: 4             #[4, 8, 12]    genera h = 10^{-k}
  relative:                 #[false, true]   # h_i = h  oppure h_i = h*|x_i|
  forward_backward: 1        # -1, 0, 1 per backward, centred, forward
  # puoi anche specificare come combinare:
  schemes:
    gradient: forward       # per ora forward va benissimo
    hessian: grad_diff      # Hessian via differenze sul gradiente

modified_newton:
  # SPD modification parameters for H
  use_sparse_exact: true
  sparse_format: dia
  spd_fix:
    lambda_init: 1e-6       # primo λ da aggiungere sulla diagonale
    lambda_factor: 10.0     # se Cholesky fallisce → λ *= factor
    lambda_max: 1e8         # sicurezza: se superi → segnali failure
    delta: 1e-12
    max_spd_tries: 10       
  hessian_source:           # da usare nel codice per decidere che Hessian prendi
    exact: hess_exact
    fd_hessian: fd_hess_from_grad
    fd_all: fd_hess_from_grad  # ma usando grad_fd invece di grad_exact

truncated_newton:
  use_precontitioning: true  # enable preconditioning in TN
  cg:
    max_iters: 50           # iterazioni interne CG (puoi testare 20, 50, 100)
    tol: 0.001            # tolerance CG -> (0.5 was way too loose -> (0.1:loose but ok, 0.01:typical, 0.001:good))
  hessvec_source:
    exact: hessvec_exact
    fd_hessian: hessvec_fd_from_grad  # usando grad_exact
    fd_all: hessvec_fd_from_grad      # usando grad_fd
  preconditioning:
    type: tridiagonal_exact  # {jacobi_hutchinson, tridiagonal_exact, dsprec}
    jacobi_hutchinson:
      num_probes: 16
      eps: 1e-8
      clip: [1e-12, 1e12]
    tridiagonal_exact:
      eps: 1e-8
      max_shift_iters: 6
    dsprec:
      delta: 1e-6
      lam: 0.0
  spd_fix:
    lambda_init: 1e-6       # primo λ da aggiungere sulla diagonale
    lambda_factor: 10.0     # se Cholesky fallisce → λ *= factor
    lambda_max: 1e8         # sicurezza: se superi → segnali failure
    max_restarts: 6

logging:
  level: info
  save_tables: true
  output_dir: output   # root di tutto

postprocessing:
  make_tables: true
  make_figures: true

  tables:
    output_dir: output
    columns:
      - problem
      - n
      - mode
      - method
      - start_id
      - f_final
      - success
      - num_iters
      - num_cg_iters
      - grad_norm
      - time
      - rate

  figures:
    output_dir: output
    top_view:
      enabled: true
      levels: 30
    rates:
      enabled: true
      use_log_scale: true
